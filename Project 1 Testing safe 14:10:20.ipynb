{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing allowed libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different paths to access data file\n",
    "\n",
    "Louise_Path = '/Users/louiseplacidet/Desktop/Machine Learning/Project 1/Git_ML_P1/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"load data.\"\"\"\n",
    "    data = np.genfromtxt(Louise_Path+\"train.csv\", delimiter=\",\",dtype='str', skip_header=1, usecols = [])\n",
    "    ind = data[:,0].astype(np.float)\n",
    "    x = data[:,2:].astype(np.float)\n",
    "    y = data[:,1]\n",
    "    return x, y, ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, ind = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 138.47 ,   51.655,   97.827, ...,    1.24 ,   -2.475,  113.497],\n",
       "       [ 160.937,   68.768,  103.235, ..., -999.   , -999.   ,   46.226],\n",
       "       [-999.   ,  162.172,  125.953, ..., -999.   , -999.   ,   44.251],\n",
       "       ...,\n",
       "       [ 105.457,   60.526,   75.839, ..., -999.   , -999.   ,   41.992],\n",
       "       [  94.951,   19.362,   68.812, ..., -999.   , -999.   ,    0.   ],\n",
       "       [-999.   ,   72.756,   70.831, ..., -999.   , -999.   ,    0.   ]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['s', 'b', 'b', ..., 's', 'b', 'b'], dtype='<U8')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1', '1', ..., '0', '1', '1'], dtype='<U8')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_binary = y.copy()\n",
    "y_binary = np.where(y_binary=='b', 1, y_binary)\n",
    "y_binary = np.where(y_binary=='s', 0, y_binary)\n",
    "y_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['s', 'b', 'b', ..., 's', 'b', 'b'], dtype='<U8')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100000., 100001., 100002., ..., 349997., 349998., 349999.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(y)\n",
    "tx = np.c_[np.ones(num_samples), x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.   ,  138.47 ,   51.655, ...,    1.24 ,   -2.475,  113.497],\n",
       "       [   1.   ,  160.937,   68.768, ..., -999.   , -999.   ,   46.226],\n",
       "       [   1.   , -999.   ,  162.172, ..., -999.   , -999.   ,   44.251],\n",
       "       ...,\n",
       "       [   1.   ,  105.457,   60.526, ..., -999.   , -999.   ,   41.992],\n",
       "       [   1.   ,   94.951,   19.362, ..., -999.   , -999.   ,    0.   ],\n",
       "       [   1.   , -999.   ,   72.756, ..., -999.   , -999.   ,    0.   ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Implementation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_mse(y, tx, w):\n",
    "    \"\"\"Calculate the loss using mse.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return 1/2 * np.mean(e**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute a gradient.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    gradient = - (1/len(e)) * tx.T.dot(e)\n",
    "    \n",
    "    return gradient, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "\n",
    "        gradient, e = compute_gradient(y,tx,w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        w = w - gamma * gradient\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    gradient = - (1/len(e)) * tx.T.dot(e)\n",
    "    return gradient, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    batch_size = 5 ##TBD\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y,tx,batch_size = batch_size, num_batches = 1):\n",
    "            gradient , e = compute_stoch_gradient(y_batch,tx_batch, w)\n",
    "            w = w - gamma*gradient\n",
    "            loss = compute_loss(y,tx,w)\n",
    "            print(\"loss\")\n",
    "            print(loss)\n",
    "            print(\"w\")\n",
    "            print(w)\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \n",
    "    w = np.linalg.solve(tx.T@tx,tx.T@y)\n",
    "    loss = compute_loss_mse(y, tx, w)\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "\n",
    "    lambdaI = 2* tx.shape[0] * lambda_ * np.eye(tx.shape[1])\n",
    "    w = np.linalg.solve(tx.T.dot(tx)+lambdaI,tx.T.dot(y))\n",
    "    loss = compute_loss_mse(y, tx, w)\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply the sigmoid function on t.\"\"\"\n",
    "    return 1 / (1+np.exp(-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating Loss for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_logistic(y, tx, w):\n",
    "    \"\"\"compute the loss: negative log likelihood.\"\"\"\n",
    "    insidesum = np.log(1 + np.exp(tx@w)) - y * (tx@w)\n",
    "    loss = len(y) * np.mean(insidesum)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Descent with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient_logistic(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    grad = tx.T @ (sigmoid(tx@w) - y)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    w = w - gamma * grad\n",
    "    \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient(y, tx, initial_w, max_iters, gamma):\n",
    "    threshold = 1e-8\n",
    "    \n",
    "    w = initial_w\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "\n",
    "        # converge criterion\n",
    "        #losses.append(loss)\n",
    "        #if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        #    break\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Newton Method for Logisitic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the Hessian of the loss function with respect to parameters w.\"\"\"\n",
    "\n",
    "    S = np.eye(len(y))\n",
    "    S_values = ( sigmoid(tx@w) * (1 - sigmoid(tx@w)))\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        S[i][i] = S_values[i]\n",
    "        \n",
    "    hessian = tx.T @ S @ tx\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_step(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and Hessian.\"\"\"\n",
    "\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    gradient = calculate_gradient(y, tx, w)\n",
    "    hessian = calculate_hessian(y, tx, w)\n",
    "    \n",
    "    return loss, gradient, hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_newton_method(y, tx, w, gamma = 1):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, gradient, hessian = logistic_regression_step(y, tx, w)\n",
    "    \n",
    "    w = w - gamma * np.linalg.inv(hessian) @ gradient\n",
    "    \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!!!!! WHY DO WE NOT CHOOSE A GAMMA FOR NEWTON METHOD????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_newton(y, tx, initial_w, max_iters, gamma):\n",
    "    threshold = 1e-8\n",
    "    w = initial_w\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_newton_method(y, tx, w, gamma)\n",
    "\n",
    "        # converge criterion\n",
    "        #losses.append(loss)\n",
    "        #if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        #    break\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regularized Gradient Descent for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient, and Hessian.\"\"\"\n",
    "    loss = calculate_loss(y, tx, w) + (lambda_ / 2) * np.sum(w**2)\n",
    "    gradient = calculate_gradient(y, tx, w) + 2 * lambda_ * w\n",
    "    hessian = calculate_hessian(y, tx, w) + 2 * lambda_\n",
    "    \n",
    "    return loss, gradient, hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, gradient, _ = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    w = w - gamma * gradient\n",
    "    \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression_gradient(y, tx, lambda_, intial_w, max_iters, gamma):\n",
    "    threshold = 1e-8\n",
    "    w = initial_w\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)# !!!!!!! WHY DO WE NOT CHOOSE A GAMMA FOR NEWTON METHOD????    \n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regularized Newton Method for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression_step(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and Hessian.\"\"\"\n",
    "\n",
    "    loss = calculate_loss(y, tx, w) + (lambda_ / 2) * np.sum(w**2)\n",
    "    gradient = calculate_gradient(y, tx, w) + 2 * lambda_ * w\n",
    "    hessian = calculate_hessian(y, tx, w) + 2 * lambda_\n",
    "    \n",
    "    return loss, gradient, hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression_newton(y, tx, initial_w, max_iters, gamma):\n",
    "    threshold = 1e-8\n",
    "    w = initial_w\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = penalized_learning_by_newton_method(y, tx, w, gamma)\n",
    "        \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Implementing the Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.array([0, 0])\n",
    "max_iters = 10\n",
    "gamma = 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_loss, SGD_w = least_squares_SGD(y_binary, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LS_loss, LS_w = least_squares(y_binary, tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LS_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LS_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_loss, ridge_w = ridge_regression(y_binary, tx, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logist_loss, logist_w = logistic_regression(y_binary, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logist_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logist_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_logist_loss, reg_logist_w = reg_logistic_regression(y_binary, tx, lambda_, intial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_logist_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_logist_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Creating Submission CVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_prediction(x_test, weights):\n",
    "    ##to be completed\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(identifier,prediction,filename):\n",
    "    submit_data = np.stack([identifier,prediction],axis=1)\n",
    "    np.savetxt(filename,submit_data,delimiter=',', fmt='%1.3f',header=\"Id,Prediction\") #fmt 1.3 means I want 3 numbers after decimal point\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4],\n",
       "       [2, 5],\n",
       "       [3, 6]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([1,2,3])\n",
    "B = np.array([4,5,6])\n",
    "C = np.stack([A,B],axis=1)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"testingsave1.csv\"\n",
    "create_submission(A,B,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
