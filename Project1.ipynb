{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizing Jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Noisy Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping features using Density Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping features with zero-variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Data with Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    median_x = np.median(x)\n",
    "    x = x - median_x\n",
    "    std_x = np.std(x)\n",
    "    x = x / std_x\n",
    "    return x, median_x, std_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path, test_path):\n",
    "    data_train = np.genfromtxt(train_path, delimiter=',', dtype='str', skip_header=1, usecols=[])\n",
    "    data_test = np.genfromtxt(test_path, delimiter=',', dtype='str', skip_header=1, usecols=[])\n",
    "    \n",
    "    x_train = data_train[:, 2:].astype(np.float)\n",
    "    x_test = data_test[:, 2:].astype(np.float)\n",
    "    \n",
    "    y_train = data_train[:, 1]\n",
    "    y_train = np.where(y_train =='s', 1, y_train)\n",
    "    y_train = np.where(y_train =='b', 0, y_train).astype(np.float)\n",
    "    y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
    "    \n",
    "    id_train = data_train[:, 0].astype(np.float)\n",
    "    id_train = np.reshape(id_train, (id_train.shape[0], 1))\n",
    "    id_train = id_train\n",
    "    id_test = data_test[:, 0].astype(np.float)\n",
    "    id_test = np.reshape(id_test, (id_test.shape[0], 1))\n",
    "    id_test = id_test\n",
    "    \n",
    "    return x_train, x_test, y_train, id_train, id_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_local_test_set(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # ***************************************************\n",
    "    # split the data based on the given ratio: \n",
    "    # ***************************************************\n",
    "\n",
    "    split_index = int(len(x)*ratio)\n",
    "    \n",
    "    indices = np.random.permutation(len(x))\n",
    "    indices_train = indices[:split_index]\n",
    "    indices_test = indices[split_index:]\n",
    "    \n",
    "    train_x = x[indices_train]\n",
    "    train_y = y[indices_train]\n",
    "    our_test_x = x[indices_test]\n",
    "    our_test_y = y[indices_test]\n",
    "    \n",
    "    return train_x, train_y, our_test_x, our_test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    phi = np.ones((x.shape[0],degree+1))\n",
    "    \n",
    "    for i in range(degree+1):\n",
    "        power_column = np.power(x,i)\n",
    "        phi[:,i] = power_column\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    x_test = x[k_indices[k]]\n",
    "    y_test = y[k_indices[k]]\n",
    "    train_indices = np.ndarray.flatten(np.delete(k_indices, k, axis=0))\n",
    "    x_train = x[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    Fi_train = build_polys(x_train, degree)\n",
    "    Fi_test = build_polys(x_test, degree)\n",
    "    \n",
    "    weights,  MSE_tr = ridge_regression(y_train, Fi_train, lambda_)\n",
    "    weights_te,  MSE_te = ridge_regression(y_test, Fi_test, lambda_)\n",
    "    \n",
    "    loss_tr = compute_rmse(y_train, Fi_train, weights)\n",
    "    loss_te = compute_rmse(y_test, Fi_test, weights)\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "degrees = 7 # Range\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-4, 0, 30)\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "# define lists to store the loss of training data and test data\n",
    "rmse_tr_f = []\n",
    "rmse_te_f = []\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    for degree in degrees:\n",
    "        rmse_tr = []\n",
    "        rmse_te = []\n",
    "        for k in range(k_fold):\n",
    "            rmse_train, rmse_test = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "            rmse_tr.append(rmse_train)\n",
    "            rmse_te.append(rmse_test)\n",
    "    rmse_tr_f.append(np.mean(rmse_tr))\n",
    "    rmse_te_f.append(np.mean(rmse_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Apply Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_regularized_loss(y, tx, w, lambda_):\n",
    "    \"\"\"compute the loss: negative log likelihood.\"\"\"\n",
    "    t = np.log(1+np.exp(tx@w)) - y*(tx@w)\n",
    "    return np.sum(t) + lambda_*(np.linalg.norm(w))**2/2\n",
    "\n",
    "def calculate_regularized_gradient(y, tx, w, lambda_):\n",
    "    return tx.T@(sigmoid(tx@w)-y) + lambda_*w\n",
    "\n",
    "def calculate_regularized_hessian(y, tx, w, lambda_):\n",
    "    S = np.diag(np.diag(sigmoid(tx@w)@(1-sigmoid(tx@w)).T))\n",
    "    return tx.T@S@tx + lambda_\n",
    "\n",
    "def regularized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient, and Hessian.\"\"\"\n",
    "    loss = calculate_regularized_loss(y, tx, w, lambda_)\n",
    "    gradient = calculate_regularized_gradient(y, tx, w, lambda_)\n",
    "#     hesh = calculate_regularized_hessian(y, tx, w, lambda_)\n",
    "    return loss, gradient\n",
    "\n",
    "def learning_by_regularized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, gradient = regularized_logistic_regression(y, tx, w, lambda_)\n",
    "    w = w - gamma*gradient\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_ , w_initial, max_iters, gamma):\n",
    "    w = w_initial\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_regularized_gradient(y, tx, w, gamma, lambda_)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 100\n",
    "threshold = 0.001\n",
    "gamma = 0.001\n",
    "lambda_ = 0.1\n",
    "losses = []\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "loss, weights = reg_logistic_regression(y, tx, lambda_, w, max_iter, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Predict Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_predict_labels(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred <= 0.5)] = -1\n",
    "    y_pred[np.where(y_pred > 0.5)] = 1\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = predict_labels(best_w, tx_test)\n",
    "our_y_predict = our_predict_labels(best_w, our_tx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate OUR Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pred = np.where()\n",
    "accuracy = (true_num/total_num)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) CSV Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "create_csv_submission(id_test,y_predict,\"fourth try\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
