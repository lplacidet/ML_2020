{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from costs import*\n",
    "from gradient_descent import*\n",
    "from plots import gradient_descent_visualization\n",
    "from logistic_regression import*\n",
    "from helpers import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path, test_path):\n",
    "    data_train = np.genfromtxt(train_path, delimiter=',', dtype='str', skip_header=1, usecols=[])\n",
    "    data_test = np.genfromtxt(test_path, delimiter=',', dtype='str', skip_header=1, usecols=[])\n",
    "    \n",
    "    x_train = data_train[:, 2:].astype(np.float)\n",
    "    x_test = data_test[:, 2:].astype(np.float)\n",
    "    \n",
    "    y_train = data_train[:, 1]\n",
    "    y_train = np.where(y_train =='s', 1, y_train)\n",
    "    y_train = np.where(y_train =='b', -1, y_train).astype(np.float)\n",
    "    y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
    "    \n",
    "    id_train = data_train[:, 0].astype(np.float)\n",
    "    id_train = np.reshape(id_train, (id_train.shape[0], 1))\n",
    "    id_train = id_train\n",
    "    id_test = data_test[:, 0].astype(np.float)\n",
    "    id_test = np.reshape(id_test, (id_test.shape[0], 1))\n",
    "    id_test = id_test\n",
    "    \n",
    "    return x_train, x_test, y_train, id_train, id_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing Outliers with Mean Feature Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixing_outliers(x):\n",
    "    outliers_indicies = np.where(x == -999.0)\n",
    "    x_cleaned = np.delete(x, outliers_indicies[0], 0)\n",
    "    mean_cleaned_cols = np.mean(x_cleaned, axis=0)\n",
    "    x[outliers_indicies] = np.take(mean_cleaned_cols, outliers_indicies[1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(x):\n",
    "    mean_x = np.mean(x)\n",
    "    x_c = x - mean_x\n",
    "    x_cov = np.cov(x_c.T)\n",
    "    eig_values, eig_vectors = np.linalg.eig(x_cov)\n",
    "    explained_variances = []\n",
    "    for i in range(len(eig_values)):\n",
    "        explained_variances.append(eig_values[i] / np.sum(eig_values))\n",
    "        if np.sum(explained_variances) >= 0.96:\n",
    "            break\n",
    "    print(np.sum(explained_variances), '\\n', explained_variances)\n",
    "    selected_vectors = eig_vectors[:len(explained_variances)]\n",
    "    x_projected = x_c@selected_vectors.T\n",
    "    return x_projected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Previous Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Dataset\n",
    "data_train_path = \"Train.csv\"\n",
    "data_test_path = \"test.csv\"\n",
    "x_train, x_test, y_train, id_train, id_test = load_data(data_train_path, data_test_path)\n",
    "\n",
    "#Replace each -999 with feature mean value\n",
    "x_train = fixing_outliers(x_train)\n",
    "x_test = fixing_outliers(x_test)\n",
    "\n",
    "#Standardize data\n",
    "x_train, mean_train, std_train = standardize(x_train)\n",
    "x_test, mean_test, std_test = standardize(x_test)\n",
    "\n",
    "#Apply PCA\n",
    "x_train_projected = apply_pca(x_train)\n",
    "x_test_projected = apply_pca(x_test)\n",
    "print(x_test_projected.shape)\n",
    "\n",
    "#Adding Offset\n",
    "tx_train = np.c_[np.ones(x_train_projected.shape[0]), x_train_projected]\n",
    "tx_test = np.c_[np.ones(x_test_projected.shape[0]), x_test_projected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Loss = 173286.79513998624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/MLProject/logistic_regression.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 Loss = -114170424.32169206\n",
      "Iteration 2 Loss = -151678615.17798772\n",
      "Iteration 3 Loss = -189186806.03428337\n",
      "Iteration 4 Loss = -226694996.8905791\n",
      "Iteration 5 Loss = -264203187.74687472\n",
      "Iteration 6 Loss = -301711378.60317034\n",
      "Iteration 7 Loss = -339219569.45946604\n",
      "Iteration 8 Loss = -376727760.31576174\n",
      "Iteration 9 Loss = -414235951.1720573\n",
      "Iteration 10 Loss = -451744142.028353\n",
      "Iteration 11 Loss = -489252332.88464874\n",
      "Iteration 12 Loss = -526760523.7409442\n",
      "Iteration 13 Loss = -564268714.59724\n",
      "Iteration 14 Loss = -601776905.4535357\n",
      "Iteration 15 Loss = -639285096.3098313\n",
      "Iteration 16 Loss = -676793287.1661272\n",
      "Iteration 17 Loss = -714301478.0224228\n",
      "Iteration 18 Loss = -751809668.8787184\n",
      "Iteration 19 Loss = -789317859.7350144\n",
      "Iteration 20 Loss = -826826050.5913098\n",
      "Iteration 21 Loss = -864334241.4476055\n",
      "Iteration 22 Loss = -901842432.3039008\n",
      "Iteration 23 Loss = -939350623.1601968\n",
      "Iteration 24 Loss = -976858814.0164924\n",
      "Iteration 25 Loss = -1014367004.872788\n",
      "Iteration 26 Loss = -1051875195.7290838\n",
      "Iteration 27 Loss = -1089383386.5853794\n",
      "Iteration 28 Loss = -1126891577.4416752\n",
      "Iteration 29 Loss = -1164399768.297971\n",
      "Iteration 30 Loss = -1201907959.1542664\n",
      "Iteration 31 Loss = -1239416150.0105624\n",
      "Iteration 32 Loss = -1276924340.866858\n",
      "Iteration 33 Loss = -1314432531.7231538\n",
      "Iteration 34 Loss = -1351940722.5794497\n",
      "Iteration 35 Loss = -1389448913.435745\n",
      "Iteration 36 Loss = -1426957104.2920406\n",
      "Iteration 37 Loss = -1464465295.1483364\n",
      "Iteration 38 Loss = -1501973486.0046322\n",
      "Iteration 39 Loss = -1539481676.8609278\n",
      "Iteration 40 Loss = -1576989867.7172236\n",
      "Iteration 41 Loss = -1614498058.5735192\n",
      "Iteration 42 Loss = -1652006249.4298148\n",
      "Iteration 43 Loss = -1689514440.2861104\n",
      "Iteration 44 Loss = -1727022631.1424065\n",
      "Iteration 45 Loss = -1764530821.9987023\n",
      "Iteration 46 Loss = -1802039012.8549976\n",
      "Iteration 47 Loss = -1839547203.7112935\n",
      "Iteration 48 Loss = -1877055394.5675893\n",
      "Iteration 49 Loss = -1914563585.4238849\n",
      "Iteration 50 Loss = -1952071776.2801805\n",
      "Iteration 51 Loss = -1989579967.1364768\n",
      "Iteration 52 Loss = -2027088157.992772\n",
      "Iteration 53 Loss = -2064596348.8490682\n",
      "Iteration 54 Loss = -2102104539.705363\n",
      "Iteration 55 Loss = -2139612730.5616589\n",
      "Iteration 56 Loss = -2177120921.417955\n",
      "Iteration 57 Loss = -2214629112.2742505\n",
      "Iteration 58 Loss = -2252137303.130546\n",
      "Iteration 59 Loss = -2289645493.986842\n",
      "Iteration 60 Loss = -2327153684.843138\n",
      "Iteration 61 Loss = -2364661875.699433\n",
      "Iteration 62 Loss = -2402170066.5557294\n",
      "Iteration 63 Loss = -2439678257.412025\n",
      "Iteration 64 Loss = -2477186448.2683206\n",
      "Iteration 65 Loss = -2514694639.1246166\n",
      "Iteration 66 Loss = -2552202829.9809127\n",
      "Iteration 67 Loss = -2589711020.837208\n",
      "Iteration 68 Loss = -2627219211.6935034\n",
      "Iteration 69 Loss = -2664727402.549799\n",
      "Iteration 70 Loss = -2702235593.4060946\n",
      "Iteration 71 Loss = -2739743784.262391\n",
      "Iteration 72 Loss = -2777251975.1186867\n",
      "Iteration 73 Loss = -2814760165.9749827\n",
      "Iteration 74 Loss = -2852268356.831278\n",
      "Iteration 75 Loss = -2889776547.6875744\n",
      "Iteration 76 Loss = -2927284738.543869\n",
      "Iteration 77 Loss = -2964792929.400165\n",
      "Iteration 78 Loss = -3002301120.256461\n",
      "Iteration 79 Loss = -3039809311.1127563\n",
      "Iteration 80 Loss = -3077317501.9690523\n",
      "Iteration 81 Loss = -3114825692.8253484\n",
      "Iteration 82 Loss = -3152333883.681644\n",
      "Iteration 83 Loss = -3189842074.5379405\n",
      "Iteration 84 Loss = -3227350265.394234\n",
      "Iteration 85 Loss = -3264858456.2505317\n",
      "Iteration 86 Loss = -3302366647.106827\n",
      "Iteration 87 Loss = -3339874837.963123\n",
      "Iteration 88 Loss = -3377383028.819418\n",
      "Iteration 89 Loss = -3414891219.675713\n",
      "Iteration 90 Loss = -3452399410.53201\n",
      "Iteration 91 Loss = -3489907601.3883057\n",
      "Iteration 92 Loss = -3527415792.2446017\n",
      "Iteration 93 Loss = -3564923983.100898\n",
      "Iteration 94 Loss = -3602432173.9571934\n",
      "Iteration 95 Loss = -3639940364.8134885\n",
      "Iteration 96 Loss = -3677448555.669783\n",
      "Iteration 97 Loss = -3714956746.52608\n",
      "Iteration 98 Loss = -3752464937.3823757\n",
      "Iteration 99 Loss = -3789973128.238672\n",
      "loss=-3827481319.0949664\n"
     ]
    }
   ],
   "source": [
    "#Apply Logistic Regression\n",
    "##Define the parameters of the algorithm\n",
    "max_iter = 100\n",
    "threshold = 0.001\n",
    "gamma = 0.001\n",
    "lambda_ = 0.1\n",
    "losses = []\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "##Start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y_train, tx_train, w, gamma)\n",
    "        print(\"Iteration \"+str(iter)+\" \"+\"Loss = \"+str(loss))\n",
    "print(\"loss={l}\".format(l=calculate_loss(y_train, tx_train, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
