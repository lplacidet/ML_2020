{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from costs import*\n",
    "from gradient_descent import*\n",
    "from plots import gradient_descent_visualization\n",
    "from logistic_regression import*\n",
    "from helpers import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Loading Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path, test_path):\n",
    "    data_train = np.genfromtxt(train_path, delimiter=',', dtype='str', skip_header=1, usecols=[])\n",
    "    data_test = np.genfromtxt(test_path, delimiter=',', dtype='str', skip_header=1, usecols=[])\n",
    "    \n",
    "    x_train = data_train[:, 2:].astype(np.float)\n",
    "    x_test = data_test[:, 2:].astype(np.float)\n",
    "    \n",
    "    y_train = data_train[:, 1]\n",
    "    y_train = np.where(y_train =='s', 1, y_train)\n",
    "    #y_train = np.where(y_train =='b', -1, y_train).astype(np.float)\n",
    "    y_train = np.where(y_train =='b', 0, y_train).astype(np.float)\n",
    "    y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
    "    \n",
    "    id_train = data_train[:, 0].astype(np.float)\n",
    "    id_train = np.reshape(id_train, (id_train.shape[0], 1))\n",
    "    id_train = id_train\n",
    "    id_test = data_test[:, 0].astype(np.float)\n",
    "    id_test = np.reshape(id_test, (id_test.shape[0], 1))\n",
    "    id_test = id_test\n",
    "    \n",
    "    return x_train, x_test, y_train, id_train, id_test\n",
    "\n",
    "#here we will use x_test as the \"data\" to input for our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_local_test_set(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # ***************************************************\n",
    "    # split the data based on the given ratio: \n",
    "    # ***************************************************\n",
    "\n",
    "    split_index = int(len(x)*ratio)\n",
    "    \n",
    "    indices = np.random.permutation(len(x))\n",
    "    indices_train = indices[:split_index]\n",
    "    indices_test = indices[split_index:]\n",
    "    \n",
    "    train_x = x[indices_train]\n",
    "    train_y = y[indices_train]\n",
    "    our_test_x = x[indices_test]\n",
    "    our_test_y = y[indices_test]\n",
    "    \n",
    "    return train_x, train_y, our_test_x, our_test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with  Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixing_outliers(x):\n",
    "    outliers_indicies = np.where(x == -999.0)\n",
    "    x_cleaned = np.delete(x, outliers_indicies[0], 0)\n",
    "    #mean_cleaned_cols = np.mean(x_cleaned, axis=0)\n",
    "    #x[outliers_indicies] = np.take(mean_cleaned_cols, outliers_indicies[1])\n",
    "    median_cleaned_cols = np.median(x_cleaned, axis = 0)\n",
    "    x[outliers_indicies] = np.take(median_cleaned_cols, outliers_indicies[1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    #mean_x = np.mean(x)\n",
    "    median_x = np.median(x)\n",
    "    #x = x - mean_x\n",
    "    x = x - median_x\n",
    "    std_x = np.std(x)\n",
    "    x = x / std_x\n",
    "    #return x, mean_x, std_x\n",
    "    return x, median_x, std_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(x):\n",
    "    mean_x = np.mean(x)\n",
    "    x_c = x - mean_x\n",
    "    x_cov = np.cov(x_c.T)\n",
    "    eig_values, eig_vectors = np.linalg.eig(x_cov)\n",
    "    explained_variances = []\n",
    "    for i in range(len(eig_values)):\n",
    "        explained_variances.append(eig_values[i] / np.sum(eig_values))\n",
    "        if np.sum(explained_variances) >= 0.96:\n",
    "            break\n",
    "    print(np.sum(explained_variances), '\\n', explained_variances)\n",
    "    selected_vectors = eig_vectors[:len(explained_variances)]\n",
    "    x_projected = x_c@selected_vectors.T\n",
    "    return x_projected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Apply Previous Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## THIS IS GASSER'S ORIGINAL VERSION\n",
    "########################################################################################\n",
    "\n",
    "#Loading Dataset\n",
    "#Louise_path = '/Users/louiseplacidet/Desktop/Machine Learning/Project 1/Git_ML_P1/Data/'\n",
    "\n",
    "#data_train_path = Louise_path + \"Train.csv\"\n",
    "#data_train_path = \"Train.csv\"\n",
    "#data_test_path = Louise_path + \"test.csv\"\n",
    "#x_train, x_test, y_train, id_train, id_test = load_data(data_train_path, data_test_path)\n",
    "\n",
    "#Replace each -999 with feature mean value\n",
    "#x_train = fixing_outliers(x_train)\n",
    "#x_test = fixing_outliers(x_test)\n",
    "\n",
    "#Standardize data\n",
    "#x_train, mean_train, std_train = standardize(x_train)\n",
    "#x_test, mean_test, std_test = standardize(x_test)\n",
    "\n",
    "#Apply PCA\n",
    "#x_train_projected = apply_pca(x_train)\n",
    "#x_test_projected = apply_pca(x_test)\n",
    "#print(x_test_projected.shape)\n",
    "\n",
    "#Adding Offset\n",
    "#tx_train = np.c_[np.ones(x_train_projected.shape[0]), x_train_projected]\n",
    "#tx_test = np.c_[np.ones(x_test_projected.shape[0]), x_test_projected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## LOUISE'S MODIFIED VERSION\n",
    "########################################################################################\n",
    "\n",
    "#Loading Dataset\n",
    "Louise_path = '/Users/louiseplacidet/Desktop/Machine Learning/Project 1/Git_ML_P1/Data/'\n",
    "\n",
    "data_train_path = Louise_path + \"Train.csv\"\n",
    "#data_train_path = \"Train.csv\"\n",
    "data_test_path = Louise_path + \"test.csv\"\n",
    "x_train, x_test, y_train, id_train, id_test = load_data(data_train_path, data_test_path)\n",
    "\n",
    "# Getting our test local set:\n",
    "x_train, y_train, x_our_test, y_our_test = create_local_test_set(x_train, y_train, 0.8,seed=1)\n",
    "\n",
    "#Replace each -999 with feature mean value\n",
    "x_train = fixing_outliers(x_train)\n",
    "\n",
    "#Standardize data\n",
    "x_train, mean_train, std_train = standardize(x_train)\n",
    "\n",
    "#Apply PCA\n",
    "x_train_projected = apply_pca(x_train)\n",
    "our_x_test_projected = apply_pca(x_our_test)\n",
    "\n",
    "#Adding Offset\n",
    "tx_train = np.c_[np.ones(x_train_projected.shape[0]), x_train_projected]\n",
    "our_tx_test = np.c_[np.ones(our_x_test_projected.shape[0]), our_x_test_projected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of x_train: (\"+str(x_train.shape[0])+\",\"+str(x_train.shape[1])+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of y_train: (\"+str(y_train.shape[0])+\",\"+str(y_train.shape[1])+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of x_our_test: (\"+str(x_our_test.shape[0])+\",\"+str(x_our_test.shape[1])+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of y_our_test: (\"+str(y_our_test.shape[0])+\",\"+str(y_our_test.shape[1])+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of tx_train: (\"+str(tx_train.shape[0])+\",\"+str(tx_train.shape[1])+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of our_tx_test: (\"+str(our_tx_test.shape[0])+\",\"+str(our_tx_test.shape[1])+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Logistic Rregression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Apply Logistic Regression\n",
    "##Define the parameters of the algorithm\n",
    "max_iter = 100\n",
    "threshold = 0.001\n",
    "gamma = 0.001\n",
    "lambda_ = 0.1\n",
    "losses = []\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "##Start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y_train, tx_train, w, gamma)\n",
    "        print(\"Iteration \"+str(iter)+\" \"+\"Loss = \"+str(loss))\n",
    "print(\"loss={l}\".format(l=calculate_loss(y_train, tx_train, w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    loss = calculate_loss(y, tx, w) + (lambda_ / 2) * np.sum(w**2)\n",
    "    gradient = calculate_gradient(y, tx, w) + 2 * lambda_ * w\n",
    "    \n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient_descent(y, tx, w, gamma, lambda_):\n",
    "\n",
    "    loss, gradient = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    w = w - gamma * gradient\n",
    "    \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Apply Regularized Logistic Regression\n",
    "##Define the parameters of the algorithm\n",
    "max_iter = 100\n",
    "threshold = 0.001\n",
    "gamma = 0.00001\n",
    "lambda_ = 0.5\n",
    "losses = []\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "##Start the regularized logistic regression\n",
    "ws = []\n",
    "losses = []\n",
    "\n",
    "for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient_descent(y_train, tx_train, w, gamma, lambda_)\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "        print(\"Iteration \"+str(iter)+\" \"+\"Loss = \"+str(loss))\n",
    "    \n",
    "best_loss = np.min(np.abs(losses))\n",
    "index_best = losses.index(best_loss)\n",
    "best_w = ws[index_best]\n",
    "\n",
    "print(\"loss={l}\".format(l=calculate_loss(y_train, tx_train, w)))\n",
    "print(\"best_loss = \"+str(best_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the Hessian of the loss function with respect to parameters w.\"\"\"\n",
    "    # ***************************************************\n",
    "    # calculate Hessian: \n",
    "    # ***************************************************\n",
    "    S = np.eye(len(y))*(np.array(( sigmoid(tx@w) * (1 - sigmoid(tx@w)))))\n",
    "        \n",
    "    hessian = tx.T @ S @ tx\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_logistic_regression(y,tx,w):\n",
    "        \n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    gradient = calculate_gradient(y, tx, w)\n",
    "    hessian = calculate_hessian(y,tx,w)\n",
    "    \n",
    "    return loss, gradient, hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_newton_method(y, tx, w, gamma = 1):\n",
    "    \n",
    "    loss, gradient, hessian = logistic_regression(y, tx, w)\n",
    "    w = w - gamma * np.linalg.inv(hessian) @ gradient\n",
    "    \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Logistic Regression with Newton Method\n",
    "##Define the parameters of the algorithm\n",
    "max_iter = 100\n",
    "threshold = 0.001\n",
    "gamma = 0.00001\n",
    "lambda_ = 0.5\n",
    "losses = []\n",
    "w = np.zeros((tx_train.shape[1], 1))\n",
    "\n",
    "##Start the logistic regression\n",
    "ws = []\n",
    "losses = []\n",
    "\n",
    "for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_newton_method(y_train, tx_train, w, gamma)\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "        print(\"Iteration \"+str(iter)+\" \"+\"Loss = \"+str(loss))\n",
    "    \n",
    "best_loss = np.min(np.abs(losses))\n",
    "index_best = losses.index(best_loss)\n",
    "best_w = ws[index_best]\n",
    "\n",
    "print(\"loss={l}\".format(l=calculate_loss(y_train, tx_train, w)))\n",
    "print(\"best_loss = \"+str(best_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III) Testing our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred <= 0)] = -1\n",
    "    y_pred[np.where(y_pred > 0)] = 1\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_predict_labels(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred <= 0.5)] = -1\n",
    "    y_pred[np.where(y_pred > 0.5)] = 1\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_y_predict = our_predict_labels(best_w, our_tx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our Model with our Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_test_loss = calculate_loss(our_y_predict, x_our_test, best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = [our_y_predict == x_our_test]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(id_test,y_predict,\"first try\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
